{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import google.colab as colab\n",
        "colab.drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VCueTgfRFujR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image, ImageSequence\n",
        "\n",
        "# Define the directory to save frames on Google Drive\n",
        "output_dir = '/content/drive/MyDrive/Dataset/child/action/frames'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to extract and save frames from a GIF\n",
        "def save_frames_from_gif(gif_path, action_label):\n",
        "    # Create an action-specific folder to save the frames\n",
        "    action_dir = os.path.join(output_dir, action_label)\n",
        "    os.makedirs(action_dir, exist_ok=True)\n",
        "\n",
        "    img = Image.open(gif_path)\n",
        "    for idx, frame in enumerate(ImageSequence.Iterator(img)):\n",
        "        frame_rgb = frame.convert('RGB')  # Convert to RGB\n",
        "        frame_filename = f\"{action_label}_frame_{idx+33}.jpg\"\n",
        "        frame_path = os.path.join(action_dir, frame_filename)\n",
        "        frame_rgb.save(frame_path)\n",
        "        print(f\"Saved: {frame_path}\")\n",
        "\n",
        "# Paths and labels for the GIF files\n",
        "actions = ['kick', 'punch', 'running', 'squat', 'stand', 'wave']\n",
        "gif_paths = [\n",
        "\n",
        "]\n",
        "\n",
        "# Extract frames for each GIF and save in respective action folders\n",
        "for gif_path, action_label in zip(gif_paths, actions):\n",
        "    save_frames_from_gif(gif_path, action_label)\n",
        "\n",
        "print(\"All frames extracted and saved successfully.\")\n",
        "\n",
        "\n",
        "# Count the number of images in each action folder\n",
        "print(\"\\nImage Counts in Each Folder:\")\n",
        "for action_label in actions:\n",
        "    action_dir = os.path.join(output_dir, action_label)\n",
        "    num_images = len([img for img in os.listdir(action_dir) if img.endswith('.jpg')])\n",
        "    print(f\"{action_label}: {num_images} images\")\n"
      ],
      "metadata": {
        "id": "6xWCX3NwDNcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow tensorflow_hub\n"
      ],
      "metadata": {
        "id": "I4soyZBuK8CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cylWVsbjFc00"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the MoveNet model (TensorFlow Hub)\n",
        "movenet_model = tf.saved_model.load('https://tfhub.dev/google/movenet/singlepose/lightning/4')\n",
        "\n",
        "# Directory for saving keypoints\n",
        "keypoints_dir = '/content/drive/MyDrive/Dataset/child/action/pose_keypoints'\n",
        "os.makedirs(keypoints_dir, exist_ok=True)\n",
        "\n",
        "# Function to detect keypoints using MoveNet\n",
        "def detect_pose(image):\n",
        "    # Convert PIL Image to NumPy array\n",
        "    image_rgb = np.array(image)\n",
        "    image_rgb = tf.convert_to_tensor(image_rgb, dtype=tf.float32)\n",
        "\n",
        "    # Resize and normalize the image (MoveNet requires 192x192 input size)\n",
        "    input_image = tf.image.resize(image_rgb, (192, 192))\n",
        "    input_image = input_image / 255.0  # Normalize image\n",
        "\n",
        "    # Run inference with MoveNet model\n",
        "    input_image = input_image[tf.newaxis,...]  # Add batch dimension\n",
        "    outputs = movenet_model(input_image)\n",
        "\n",
        "    # Extract keypoints (pose landmarks) from MoveNet's output\n",
        "    keypoints = outputs['output_0'].numpy()\n",
        "    keypoints = keypoints.squeeze()  # Remove batch dimension\n",
        "\n",
        "    # Extract keypoints for x, y, confidence (visibility)\n",
        "    keypoints = keypoints[:, :3]  # We are only interested in x, y, and confidence\n",
        "    return keypoints\n",
        "\n",
        "# Function to process and save keypoints for all frames in action folders\n",
        "def save_keypoints_for_action(action_folder, action_label):\n",
        "    keypoints_data = []\n",
        "\n",
        "    # Retrieve all image files from the action-specific folder\n",
        "    for filename in sorted(os.listdir(action_folder)):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Supported image formats\n",
        "            image_path = os.path.join(action_folder, filename)\n",
        "            frame = Image.open(image_path).convert('RGB')  # Convert to RGB\n",
        "\n",
        "            # Detect pose keypoints\n",
        "            keypoints = detect_pose(frame)\n",
        "            keypoints_data.append(keypoints.tolist())\n",
        "\n",
        "    # Save the keypoints with the action label\n",
        "    data = {\n",
        "        'label': action_label,\n",
        "        'keypoints': keypoints_data\n",
        "    }\n",
        "\n",
        "    # Save data as JSON\n",
        "    json_path = os.path.join(keypoints_dir, f\"{action_label}_keypoints.json\")\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "# Define the action directories\n",
        "actions = ['kick', 'punch', 'squat', 'stand', 'wave', 'running', 'sit', 'fall']\n",
        "action_folders = [\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/kick',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/punch',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/squat',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/stand',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/wave',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/running',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/fall',\n",
        "    '/content/drive/MyDrive/Dataset/child/action/frames/sit'\n",
        "]\n",
        "\n",
        "# Process each action folder and generate keypoints JSON\n",
        "for action_folder, action_label in zip(action_folders, actions):\n",
        "    save_keypoints_for_action(action_folder, action_label)\n",
        "\n",
        "print(\"Keypoint extraction completed for all actions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Function to visualize keypoints on an image (modified for MediaPipe)\n",
        "def visualize_keypoints(image, keypoints):\n",
        "    plt.imshow(image)\n",
        "    image_width, image_height = image.size\n",
        "    for i, keypoint in enumerate(keypoints):\n",
        "        x, y, z, visibility = keypoint\n",
        "        if visibility > 0.5:  # Show only keypoints with visibility > 0.5\n",
        "            plt.scatter(x * image_width, y * image_height, s=10, c='red', marker='o')\n",
        "            plt.text(x * image_width, y * image_height, str(i), color=\"blue\", fontsize=8)\n",
        "    plt.axis('off')\n",
        "\n",
        "# Directory paths\n",
        "keypoints_dir = '/content/drive/MyDrive/Dataset/child/action/pose_keypoints'\n",
        "frame_folders = {\n",
        "    'kick': '/content/drive/MyDrive/Dataset/child/action/frames/kick',\n",
        "    'punch': '/content/drive/MyDrive/Dataset/child/action/frames/punch',\n",
        "    'squat': '/content/drive/MyDrive/Dataset/child/action/frames/squat',\n",
        "    'stand': '/content/drive/MyDrive/Dataset/child/action/frames/stand',\n",
        "    'wave': '/content/drive/MyDrive/Dataset/child/action/frames/wave',\n",
        "    'running': '/content/drive/MyDrive/Dataset/child/action/frames/running',\n",
        "    'fall': '/content/drive/MyDrive/Dataset/child/action/frames/fall',\n",
        "    'sit':  '/content/drive/MyDrive/Dataset/child/action/frames/sit'\n",
        "}\n",
        "\n",
        "# Function to load and visualize keypoints for each frame in a given class folder\n",
        "def visualize_keypoints_for_class(class_name, frame_folder, keypoints_path):\n",
        "    # Load keypoints JSON data\n",
        "    with open(keypoints_path, 'r') as f:\n",
        "        keypoints_data = json.load(f)\n",
        "\n",
        "    # Iterate through frames in the directory\n",
        "    frame_files = sorted([file for file in os.listdir(frame_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Display up to 10 frames with keypoints\n",
        "    for i, frame_file in enumerate(frame_files[:10]):\n",
        "        frame_path = os.path.join(frame_folder, frame_file)\n",
        "        frame = Image.open(frame_path).convert('RGB')\n",
        "\n",
        "        # Get keypoints for the current frame\n",
        "        keypoints = keypoints_data['keypoints'][i]\n",
        "\n",
        "        # Visualize keypoints on the current frame\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        visualize_keypoints(frame, keypoints)\n",
        "        plt.title(f\"Class: {class_name} - Frame {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "# Visualize keypoints for each action class\n",
        "for class_name, frame_folder in frame_folders.items():\n",
        "    keypoints_path = os.path.join(keypoints_dir, f\"{class_name}_keypoints.json\")\n",
        "    visualize_keypoints_for_class(class_name, frame_folder, keypoints_path)\n"
      ],
      "metadata": {
        "id": "fHe0l-4OPVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Function to create subsequences from a sequence\n",
        "def create_subsequences(sequence, window_size, step_size):\n",
        "    subsequences = []\n",
        "    for start in range(0, len(sequence) - window_size + 1, step_size):\n",
        "        end = start + window_size\n",
        "        subsequences.append(sequence[start:end])\n",
        "    return subsequences\n",
        "\n",
        "# Load keypoints from JSON files and create subsequences\n",
        "def load_keypoints(keypoints_dir, window_size=30, step_size=5):\n",
        "    X = []\n",
        "    y = []\n",
        "    for filename in os.listdir(keypoints_dir):\n",
        "        if filename.endswith('.json'):\n",
        "            filepath = os.path.join(keypoints_dir, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                keypoints = data['keypoints']\n",
        "                label = data['label']\n",
        "\n",
        "                # Flatten the keypoints for each frame\n",
        "                flattened_keypoints = [np.array(kp).flatten() for kp in keypoints]\n",
        "\n",
        "                # Create subsequences\n",
        "                subsequences = create_subsequences(flattened_keypoints, window_size, step_size)\n",
        "                X.extend(subsequences)\n",
        "                y.extend([label] * len(subsequences))\n",
        "\n",
        "    # Convert X and y to arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Load keypoints data\n",
        "X, y = load_keypoints(keypoints_dir)\n",
        "\n",
        "# Convert labels to numerical values\n",
        "actions = ['kick', 'punch', 'squat', 'stand', 'wave', 'running', 'sit', 'fall']\n",
        "action_map = {action: i for i, action in enumerate(actions)}\n",
        "y_numeric = np.array([action_map[label] for label in y])\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_encoded = to_categorical(y_numeric, num_classes=len(actions))\n",
        "\n",
        "# Split the data into training and test sets using stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_numeric\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "bYbhAm-7MfcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU, Bidirectional\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(len(actions), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/Dataset/child/action/action_recognition_model.h5')\n"
      ],
      "metadata": {
        "id": "MO5lil7pMi_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predict the action for a new sequence of keypoints\n",
        "new_keypoints = X_test[0].reshape(1, X_test.shape[1], X_test.shape[2])\n",
        "predicted_action = model.predict(new_keypoints)\n",
        "predicted_label = actions[np.argmax(predicted_action)]\n",
        "print(f\"Predicted Action: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "5kGl_tGbNAZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predict actions for all test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=actions, yticklabels=actions)\n",
        "plt.xlabel('Predicted Action')\n",
        "plt.ylabel('True Action')\n",
        "plt.title('Confusion Matrix for Action Recognition')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IkICbjlKNBxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')  # Place legend in the lower right corner\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')  # Place legend in the upper right corner\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7JAMkY3tMFNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/Dataset/child/action/action_recognition_model.h5')\n",
        "\n",
        "# Load the MoveNet model to extract keypoints\n",
        "import tensorflow_hub as hub\n",
        "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "\n",
        "# Function to detect keypoints for a single image\n",
        "def detect_pose(image):\n",
        "    input_image = tf.image.resize_with_pad(np.expand_dims(image, axis=0), 256, 256)\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    keypoints_with_scores = movenet.signatures['serving_default'](input_image)\n",
        "    keypoints = keypoints_with_scores['output_0'].numpy()[0, 0, :, :]\n",
        "    return keypoints[:, :2]  # Extract only x and y coordinates\n",
        "\n",
        "# Load and preprocess a test image\n",
        "image_path = '/content/drive/MyDrive/Dataset/child/action/frames/running/running_frame_0.jpg'  # Replace with your test image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image_np = np.array(image)\n",
        "\n",
        "# Detect keypoints\n",
        "keypoints = detect_pose(image_np)\n",
        "\n",
        "# Prepare the keypoints for model input\n",
        "# Ensure that keypoints are reshaped to match the input shape of the model\n",
        "# If your model expects (timesteps, features), add a new axis if necessary\n",
        "keypoints = np.expand_dims(keypoints, axis=0)  # Add batch dimension\n",
        "keypoints = np.expand_dims(keypoints, axis=0)  # Add time dimension if needed\n",
        "\n",
        "# Predict the action\n",
        "predictions = model.predict(keypoints)\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# Map the prediction to class label\n",
        "action_labels = ['kick', 'punch', 'squat', 'stand', 'wave', 'running', 'fall', 'sit']  # Update with your actual labels\n",
        "predicted_label = action_labels[predicted_class]\n",
        "\n",
        "print(f\"The model predicts this action as: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "wjzJXZKkUyG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install MediaPipe if not already installed\n",
        "!pip install mediapipe\n",
        "\n",
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "syFMRKCddK2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the image\n",
        "image_path = '/content/drive/MyDrive/Dataset/child/action/frames/stand/stand_frame_102.jpg'  # Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils  # For drawing keypoints and skeleton\n",
        "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "\n",
        "# Process the image and extract keypoints\n",
        "results = pose.process(image_rgb)\n",
        "\n",
        "# Create a copy of the image to draw on\n",
        "image_with_keypoints = image_rgb.copy()\n",
        "\n",
        "# Check if any landmarks are detected\n",
        "if not results.pose_landmarks:\n",
        "    print(\"No pose landmarks detected.\")\n",
        "    keypoints = np.zeros((33, 3))  # MediaPipe provides 33 landmarks\n",
        "else:\n",
        "    # Draw pose landmarks on the image\n",
        "    mp_drawing.draw_landmarks(\n",
        "        image_with_keypoints,\n",
        "        results.pose_landmarks,\n",
        "        mp_pose.POSE_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
        "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2))\n",
        "\n",
        "    # Extract keypoints\n",
        "    landmarks = results.pose_landmarks.landmark\n",
        "    keypoints = []\n",
        "    for landmark in landmarks:\n",
        "        x = landmark.x  # Normalized x-coordinate\n",
        "        y = landmark.y  # Normalized y-coordinate\n",
        "        visibility = landmark.visibility  # Confidence score\n",
        "        keypoints.append([y, x, visibility])\n",
        "    keypoints = np.array(keypoints)\n",
        "\n",
        "# Flatten keypoints\n",
        "keypoints_flatten = keypoints.flatten()\n",
        "\n",
        "# Define window size (should match the window_size used during training)\n",
        "window_size = 30\n",
        "\n",
        "# Create a sequence by replicating the keypoints\n",
        "sequence = np.array([keypoints_flatten] * window_size)\n",
        "\n",
        "# Reshape to match the input shape expected by the model\n",
        "sequence = np.expand_dims(sequence, axis=0)  # Shape: (1, window_size, num_features)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/Dataset/child/action/action_recognition_model.h5')\n",
        "\n",
        "# Get the prediction probabilities\n",
        "prediction = model.predict(sequence)\n",
        "\n",
        "# Get the action with the highest probability\n",
        "predicted_action_index = np.argmax(prediction[0])\n",
        "\n",
        "# Define the action labels (should match the labels used during training)\n",
        "actions = ['kick', 'punch', 'squat', 'stand', 'wave', 'running', 'sit', 'fall']\n",
        "\n",
        "# Get the predicted action label\n",
        "predicted_action = actions[predicted_action_index]\n",
        "\n",
        "# Get the confidence score of the prediction\n",
        "confidence_score = prediction[0][predicted_action_index]\n",
        "\n",
        "# Choose font and position for the text\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "position = (10, 30)  # Adjust as needed\n",
        "font_scale = 1\n",
        "font_color = (255, 0, 0)  # Blue color in BGR\n",
        "thickness = 2\n",
        "\n",
        "# Prepare the text\n",
        "text = f\"Predicted Action: {predicted_action} ({confidence_score*100:.2f}%)\"\n",
        "\n",
        "# Convert image back to BGR for OpenCV\n",
        "image_with_keypoints_bgr = cv2.cvtColor(image_with_keypoints, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Put text on the image\n",
        "cv2.putText(image_with_keypoints_bgr, text, position, font, font_scale, font_color, thickness, cv2.LINE_AA)\n",
        "\n",
        "# Convert image back to RGB for Matplotlib\n",
        "final_image = cv2.cvtColor(image_with_keypoints_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(final_image)\n",
        "plt.axis('off')\n",
        "plt.title('Action Recognition Result')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eS2lZixfcpa7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}